/*
 * Copyright (2023) The Delta Lake Project Authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package io.delta.kernel;

import java.net.URI;
import java.util.*;

import io.delta.kernel.client.TableClient;
import io.delta.kernel.data.*;
import io.delta.kernel.expressions.Literal;
import io.delta.kernel.types.StructType;
import io.delta.kernel.utils.CloseableIterator;
import io.delta.kernel.utils.DataFileStatus;

import io.delta.kernel.internal.TransactionImpl;
import io.delta.kernel.internal.actions.AddFile;
import io.delta.kernel.internal.actions.SingleAction;
import io.delta.kernel.internal.data.GenericRow;
import io.delta.kernel.internal.data.TransactionStateRow;
import io.delta.kernel.internal.fs.Path;
import io.delta.kernel.internal.util.ColumnMapping;
import io.delta.kernel.internal.util.PartitionUtils;
import static io.delta.kernel.internal.fs.FileOperations.tryRelativizePath;

public interface Transaction {
    /**
     * Get the state of the transaction. The state contains the following: - logical schema -
     * physical schema, - cm mode - table properties (such as generated CDF etc.)
     */
    Row getState(TableClient tableClient);

    /**
     * Get the list of logical names of the partition columns. This helps the connector to do
     * physical partitioning of the data before asking the Kernel to write the data.
     */
    List<String> getPartitionColumns(TableClient tableClient);

    /**
     * Get the expected output of the data that is being written to the table.
     */
    StructType getSchema(TableClient tableClient);

    /**
     * Commit the transaction including the staged data rows generated by either or both
     * `Transaction.stageAppendOnlyData` or `Transaction.stageMergeData`.
     *
     * @param tableClient
     * @param stagedData           Iterator of data actions to commit. These data actions are
     *                             generated by the `Transaction.stageAppendOnlyData` or manually
     *                             created. Manual creation of data actions is useful when has the
     *                             data files and want to directly inject them into the table. One
     *                             important note is that the connector should be able to provide
     *                             the same content if the transaction commit fails and needs to be
     *                             retried.
     * @param previousCommitStatus If the transaction is retried, return the previous status of the
     *                             transaction. This helps the Kernel reuse the conflict resolution
     *                             work done in the previous attempt.
     * @return {@link TransactionCommitStatus} status of the successful transaction. If not
     * successful, returns a state with which the transaction can be retried for commit again.
     * @throws TransactionNonRetryableConflictException when the transaction has encountered a
     *                                                  non-retryable conflicts and needs the
     *                                                  updates to be regenerated on top of the
     *                                                  latest changes in the table.
     */
    TransactionCommitStatus commit(
            TableClient tableClient,
            CloseableIterator<Row> stagedData,
            Optional<TransactionCommitStatus> previousCommitStatus
    ) throws TransactionNonRetryableConflictException;

    /**
     * Given the logical data that needs to be written to the table, convert it into the required
     * physical data depending upon the table Delta protocol options. Kernel takes care of adding
     * any additional column or removing existing columns that doesn't need to be in physical data
     * files. All these transformations are driven by the Delta protocol and table features enabled
     * on the table.
     * <p>
     * The data converted is per partition. Partition values are provided as map of column name to
     * partition value (as {@link Literal}). If the table is an un-partitioned table, then map
     * should be empty.
     *
     * @param tableClient      {@link TableClient} instance to use.
     * @param transactionState The transaction state
     * @param dataIter         Iterator of logical data to transform to physical data. All the data
     *                         in this iterator should belong to one physical partition.
     * @param partitionValues  The partition values for the data. If the table is un-partitioned,
     *                         the map should be empty
     * @return Iterator of physical data to write to the data files.
     */
    static CloseableIterator<FilteredColumnarBatch> transformLogicalData(
            TableClient tableClient,
            Row transactionState,
            CloseableIterator<FilteredColumnarBatch> dataIter,
            Map<String, Literal> partitionValues) {
        // TODO: add support for:
        // - enforcing the constraints
        // - generating the default value columns
        // - generating the generated columns

        // Remove the partition columns from the data as they are already part of file metadata
        // and are not needed in the data files.
        CloseableIterator<FilteredColumnarBatch> dataWithPartsIter = dataIter.map(
                filteredColumnarBatch -> {
                    ColumnarBatch data = filteredColumnarBatch.getData();
                    for (String partitionColName : partitionValues.keySet()) {
                        int partitionColIndex = data.getSchema().indexOf(partitionColName);
                        data = data.withDeletedColumnAt(partitionColIndex);
                    }
                    return new FilteredColumnarBatch(data,
                            filteredColumnarBatch.getSelectionVector());
                }
        );

        String columnMappingMode = TransactionStateRow.getColumnMappingMode(transactionState);
        switch (columnMappingMode) {
            case ColumnMapping.COLUMN_MAPPING_MODE_NONE:
                return dataWithPartsIter;
            case ColumnMapping.COLUMN_MAPPING_MODE_NAME: // fall through
            case ColumnMapping.COLUMN_MAPPING_MODE_ID:
                StructType physicalSchema =
                        TransactionStateRow.getPhysicalSchema(tableClient, transactionState);
                return dataWithPartsIter.map(
                        filteredColumnarBatch ->
                                new FilteredColumnarBatch(
                                        filteredColumnarBatch.getData()
                                                .withNewSchema(physicalSchema),
                                        filteredColumnarBatch.getSelectionVector())
                );
            default:
                throw new UnsupportedOperationException(
                        "Unsupported column mapping mode: " + columnMappingMode);
        }
    }

    /**
     * Get the context for writing data into a table. The context tells the connector where the data
     * should be written and what should be the target file size.
     *
     * @param tableClient
     * @param transactionState
     * @param partitionValues
     * @return
     */
    static WriteContext getWriteContext(
            TableClient tableClient,
            Row transactionState,
            Map<String, Literal> partitionValues) {
        return new WriteContext(
                TransactionImpl.getTargetDirectory(transactionState, partitionValues),
                TransactionStateRow.getTargetFileSize(transactionState),
                partitionValues,
                TransactionImpl.getStatisticsColumns(tableClient, transactionState));
    }

    /**
     * Stage the given data files as a Delta actions that can be committed in a transaction.
     *
     * @param tableClient      {@link TableClient} instance.
     * @param transactionState State of the transaction.
     * @param fileStatusIter   Iterator of row objects representing each data file written. The
     *                         schema of the {@link Row} is as follows:
     *                         <ol>
     *                         <li><ul>
     *                         <li>name: file_path</li>
     *                         <li>type: String</li>
     *                         <li>description: Fully qualified path of the data file.</li>
     *                         </ul></li>
     *                         <li><ul>
     *                         <li>name: size</li>
     *                         <li>type: long</li>
     *                         <li>description: Size of the data file in bytes</li>
     *                         </ul></li>
     *                         <li><ul>
     *                         <li>name: modificationTime</li>
     *                         <li>type: long</li>
     *                         <li>description: Data file last modification time in millis.</li>
     *                         </ul></li>
     *                         <li><ul>
     *                         <li>name: stats</li>
     *                         <li>type: struct</li>
     *                         <li>description: Column level statistics of subset of columns in
     *                         the data file.
     *                         <ul>
     *                         <li><ul>
     *                         <li>name: numRecords</li>
     *                         <li>type: long</li>
     *                         <li>description: Number of rows in the data file</li>
     *                         </ul></li>
     *                         <li><ul>
     *                         <li>name: minValues</li>
     *                         <li>type: struct</li>
     *                         <li>description: Minimum value of each leaf level column</li>
     *                         </ul></li>
     *                         <li><ul>
     *                         <li>name: maxValues</li>
     *                         <li>type: struct</li>
     *                         <li>description: Maximum value of each leaf level column</li>
     *                         </ul></li>
     *                         <li><ul>
     *                         <li>name: nullCount</li>
     *                         <li>type: struct</li>
     *                         <li>description: Null count of each leaf level column</li>
     *                         </ul></li>
     *                         </ul>
     *                         </ul></li>
     *                         </ol>
     * @param writeContext     The context used when writing the data files given in
     *                         {@code fileStatusIter}
     * @return
     */
    static CloseableIterator<Row> stageAppendOnlyData(
            TableClient tableClient,
            Row transactionState,
            CloseableIterator<DataFileStatus> fileStatusIter,
            WriteContext writeContext) {
        URI tableRoot = new Path(TransactionStateRow.getTableRoot(transactionState)).toUri();
        return fileStatusIter.map(
                fileWriteResult -> {
                    Path filePath = new Path(fileWriteResult.getPath());
                    Map<Integer, Object> valueMap = new HashMap<>();
                    valueMap.put(0, tryRelativizePath(filePath, tableRoot).toString()); // path

                    MapValue partitionValues =
                            PartitionUtils.serializePartitionMap(writeContext.getPartitionValues());
                    valueMap.put(1, partitionValues);
                    valueMap.put(2, fileWriteResult.getSize());
                    valueMap.put(3, fileWriteResult.getModificationTime());
                    valueMap.put(4, true); // dataChange
                    valueMap.put(5, null); // deletionVector
                    if (fileWriteResult.getStatistics().isPresent()) {
                        valueMap.put(6, fileWriteResult.getStatistics().get().serializeAsJson());
                    }

                    Row addFileRow = new GenericRow(AddFile.SCHEMA_WITH_STATS, valueMap);
                    return SingleAction.createAddFileSingleAction(addFileRow);
                }
        );
    }
}
